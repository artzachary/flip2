{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### IMPORT ######################################\n",
    "# package listing                                                                 #\n",
    "###################################################################################\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import csv as csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import mechanicalsoup\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    while True:\n",
    "        #################################### GET DATA #####################################\n",
    "        # Get all available Roster Update data                                            #\n",
    "        ###################################################################################\n",
    "\n",
    "        # initialize player current price & profit\n",
    "        df=None\n",
    "        code=[] #player ID\n",
    "        name=[] #player name\n",
    "        rating=[] #player rating\n",
    "        buy=[] #card buy now\n",
    "        sell=[] #card sell now\n",
    "        profit=[] #card profit\n",
    "\n",
    "        url = 'https://mlb19.theshownation.com/community_market'\n",
    "        prefix = 'https://mlb19.theshownation.com/community_market?page='\n",
    "        suffix = '&type_id=0'\n",
    "        page1 = requests.get(url)\n",
    "        soup1 = BeautifulSoup(page1.text,'html.parser')\n",
    "\n",
    "        # index\n",
    "        max_ind = soup1.find('h3').get_text().split(' ')[6]\n",
    "\n",
    "        for i in range(int(max_ind)):\n",
    "            page1 = requests.get(prefix + str(i) + suffix)\n",
    "            soup2 = BeautifulSoup(page1.text,'html.parser')\n",
    "\n",
    "            # soup\n",
    "            list_tags = soup2.find('div').find_all('td')\n",
    "            a_tags = soup2.select(\"a[href*=community_market]\")\n",
    "\n",
    "            a_tags.pop(len(a_tags)-1)\n",
    "\n",
    "            for l in range(0,2):\n",
    "                a_tags.pop(l)\n",
    "\n",
    "            # ids\n",
    "            for j in range(0,int(len(a_tags)/2-1)):\n",
    "                code.append(a_tags[j*2+1].get('href').split(\"/\")[3])\n",
    "\n",
    "            # names, prices\n",
    "            for k in range(0,int(len(list_tags)/9-1)):\n",
    "                name.append(list_tags[k*9+1].get_text().strip())\n",
    "                rating.append(int(list_tags[k*9+2].get_text().strip()))\n",
    "                buy.append(int(list_tags[k*9+3].get_text()))\n",
    "                sell.append(int(list_tags[k*9+4].get_text()))\n",
    "\n",
    "        maxprofit = lambda x,y: 0.9*int(x)-int(y)\n",
    "        profit = map(maxprofit,buy,sell)\n",
    "\n",
    "        df = pd.DataFrame(zip(name,rating,buy,sell,profit,code),columns=['Name','Rating','Buy','Sell','Profit','Code'])\n",
    "\n",
    "        if name:\n",
    "            name.clear(), rating.clear(), buy.clear(), sell.clear(), code.clear()\n",
    "\n",
    "        ################################### FILTERING #####################################\n",
    "        # in order to cut down the number of records, apply filters here                  #\n",
    "        # store original results in copy                                                  #\n",
    "        ###################################################################################\n",
    "\n",
    "        #FILTER \n",
    "        #in order to cut down the number of records, apply filters here\n",
    "        #store original results in copy\n",
    "        orig=df\n",
    "\n",
    "        #filter, sort\n",
    "        df=df[(df['Rating']>70) & (df['Profit']>1000)].sort_values('Profit',ascending=False).reset_index(drop=True)\n",
    "\n",
    "        ############################### HISTORICAL PRICES #################################\n",
    "        # using df from above, ping cards by code and find latest buy/sell prices,        #\n",
    "        ###################################################################################\n",
    "\n",
    "        # initilaize historical price\n",
    "        datesf=None\n",
    "        hprice=[] #historical price df\n",
    "        hdt=[] #historical datetime df\n",
    "        hdate=[] #historical date\n",
    "        codef=[]\n",
    "\n",
    "        player_url = 'https://mlb19.theshownation.com/community_market/listings/'\n",
    "\n",
    "        for j in range(0,int(len(df)-1)): ## use this logic to assemble the entire card list\n",
    "\n",
    "            suffix = df['Code'][j]\n",
    "            player = requests.get(player_url+suffix)\n",
    "            player_soup = BeautifulSoup(player.text,'html.parser')\n",
    "\n",
    "            try:\n",
    "                record_tags = player_soup.find('table', id='table-completed-orders').find_all('td')\n",
    "            except:\n",
    "                continue\n",
    "            ##record_tags[2n: 0_to_50] = prices\n",
    "            ##record_tags[2n+1: 0_to_50] = times\n",
    "\n",
    "            #experiment with expanding this - the site suggests only 50 prices are available but \n",
    "            #it seems like there may be no limit in terms of price\n",
    "            for i in range(0,50):\n",
    "                try:\n",
    "                    #strip outside newlines, split by interior newlines and take str index 2 (the amount)\n",
    "                    #apply translation replacing comma w/ null to put string number in form python can \n",
    "                    #interpret as int in order to utilize \n",
    "                    hprice.append(int(record_tags[2*i].get_text().strip().split('\\n')[2].translate(str.maketrans({',':''}))))\n",
    "\n",
    "                    #create datetime object for each sale, this will let us do math to dates/times\n",
    "                    #have to remove PDT time zone reference because datetime can't parse this correctly\n",
    "                    #instead, just add 2 hours to make up for the time difference for easy review\n",
    "                    hdt.append(datetime.strptime(record_tags[2*i +1].get_text().replace(' PDT',''), '%m/%d/%Y %I:%M%p') + timedelta(hours=2))\n",
    "\n",
    "                    #append readable date \n",
    "                    hdate.append(hdt[i].strftime('%m/%d/%Y %I:%M%p'))\n",
    "                    codef.append(suffix)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            datesf = pd.DataFrame(zip(codef,hprice,hdate,hdt),columns=['Code','Price','Date','DateTime'])\n",
    "\n",
    "            if hprice:\n",
    "                hprice.clear, hdate.clear, hdt.clear\n",
    "\n",
    "        ################################### CLUSTERING ####################################\n",
    "        # we have 2 clusters, quick buy & order buy, simple way to detect which is which: #\n",
    "        # k-means clustering. i do that here                                              #\n",
    "        ###################################################################################\n",
    "\n",
    "        datesf2=None\n",
    "        datesf2=datesf\n",
    "        unique_code = []\n",
    "\n",
    "        # need a unique UNSORTED list of codes\n",
    "        data = datesf2['Code']\n",
    "        seen = set()\n",
    "\n",
    "        for x in datesf2['Code']:\n",
    "            if x not in seen:\n",
    "                unique_code.append(x)\n",
    "                seen.add(x)\n",
    "\n",
    "        for code in unique_code:\n",
    "            # create kmeans object\n",
    "            kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "            # fit kmeans object to data\n",
    "            kmeans.fit(datesf2.loc[datesf2['Code']==code][['Price']])\n",
    "\n",
    "            # make sure that the cluster associated w/ quick buy is always = 1\n",
    "            idx = np.argsort(kmeans.cluster_centers_.sum(axis=1))\n",
    "            lut = np.zeros_like(idx)\n",
    "            lut[idx] = np.arange(2)\n",
    "\n",
    "            # append clusters to indicate whether something is a quickbuy or an order\n",
    "            y_km = lut[kmeans.labels_]\n",
    "\n",
    "            datesf2.loc[datesf2['Code']==code,'QB'] = y_km\n",
    "\n",
    "        ################################### LIQUIDITY #####################################\n",
    "        # we have 2 clusters, quick buy & order buy, simple way to detect which is which: #\n",
    "        # k-means clustering. i do that here                                              #\n",
    "        ###################################################################################\n",
    "\n",
    "        # caveat: observing a small sample (perhaps should expand view?) of a complex stoch. \n",
    "        # process and trying to apply simple models to it. metrics are simple\n",
    "        # goal: improving the capital return & efficiency in terms of time spent adjusting buys & sells\n",
    "\n",
    "        # QB is similar to a poisson process: generally reasonable to assume inter-arrival times \n",
    "        # are consistent buy-to-buy given that the process which generates events should be IID\n",
    "        # really though, this model is useful as it gives us a way to judge liquidity on QB mkt\n",
    "\n",
    "        # we can consider metrics for evaluating liqudity: \n",
    "        ### how many QBs out of last 100 sales? \n",
    "        #### an ideal flip candidate would have a higher QB ratio\n",
    "        ### what is QB inter-arrival time (average) over last 100 sales?\n",
    "        #### an ideal flip candidate would have a smaller inter-arrival value for QB\n",
    "        #### using this information, we can potentially forecast future arrival for QBs to anticipate\n",
    "        #### QB behavior and be positioned\n",
    "\n",
    "        basef=None ## base data for liq. metrics\n",
    "        metrics=None ## hold the metrics\n",
    "        codes=[]\n",
    "        lam=[]\n",
    "        qbtt=[]\n",
    "        diff=[]\n",
    "\n",
    "        # collect base data & reset index\n",
    "        basef = datesf2[datesf2['QB']==1].reset_index(drop=True)\n",
    "        ind = 0\n",
    "\n",
    "        # determine time between QBs\n",
    "        for code in unique_code:\n",
    "\n",
    "            ## initialize the list for every Code\n",
    "            if diff:\n",
    "                diff.clear()\n",
    "            else:\n",
    "                diff=[]\n",
    "\n",
    "            # apply diff down the ladder of QB times until the last \n",
    "            for i in range(ind,ind + len(basef.loc[basef['Code']==code])-1): \n",
    "                tdiff = (basef['DateTime'][i] - basef['DateTime'][i+1]).total_seconds()/60\n",
    "                diff.append(int(tdiff))\n",
    "\n",
    "            ind = ind + len(basef.loc[basef['Code']==code])\n",
    "\n",
    "            # insert a 0 for the final entry of each code\n",
    "            diff.insert(len(diff),0)\n",
    "\n",
    "            # push diff list back to the base data by-code\n",
    "            basef.loc[basef['Code']==code,'Diff'] = diff\n",
    "\n",
    "            ## using above metrics, calculate some additional statistics:\n",
    "            # average inter-arrival time (Poisson lambda)\n",
    "            lsum = sum(diff)/len(diff)\n",
    "            lam.append(lsum)\n",
    "\n",
    "            # proportion of quick-buys to total orders ~ relative frequency \n",
    "            qsum = sum(basef.loc[basef['Code']==code]['QB'])/100\n",
    "            qbtt.append(qsum)\n",
    "\n",
    "        metrics = pd.DataFrame(zip(unique_code, lam, qbtt), columns=['Code','Lambda','QBR'])\n",
    "\n",
    "        # inner join back w/ original \n",
    "        df_inner = pd.merge(df, metrics, on='Code', how='inner')\n",
    "\n",
    "        base_url = 'https://mlb19.theshownation.com/community_market/listings/'\n",
    "\n",
    "        df_inner[\"URL\"] = base_url + df_inner['Code']\n",
    "\n",
    "        #df_inner[(df_inner[\"Sell\"]>0) & (df_inner[\"Lambda\"]>0)].sort_values('Lambda')\n",
    "        df_inner[(df_inner[\"Sell\"]>0) & (df_inner['Lambda']>0) & (df_inner['Lambda']<60) & (df_inner['Profit']>1000)].sort_values('Lambda').to_csv(\"buys.csv\")\n",
    "        \n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_work(150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
